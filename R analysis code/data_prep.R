
# Contact: Maya Mathur (mmathur@stanford.edu)

############################### SET UP ###############################

rm(list=ls())

##### Set your directories here #####
# location of code
code.dir = "~/Dropbox/Personal computer/Independent studies/Uncanny Valley III (UV3)/Qualtrics Mousetracker (OSF)/R analysis code"

# location of raw Qualtrics data
data.dir = "~/Dropbox/Personal computer/Independent studies/Uncanny Valley III (UV3)/Qualtrics Mousetracker (OSF)/Validation study/Data"

# where to save key
key.dir = "~/Dropbox/Personal computer/Independent studies/Uncanny Valley III (UV3)/Qualtrics Mousetracker (OSF)/Validation study"

# where to save results
results.dir = "~/Dropbox/Personal computer/Independent studies/Uncanny Valley III (UV3)/Qualtrics Mousetracker (OSF)/Validation study/Results"


# load helper fns
setwd(code.dir)
source("general_helper.R")

# disable scientific notation because causes big numbers to be treated as equal
options(scipen=999)

# load raw, wide-format Qualtrics data
setwd(data.dir)
d = read.csv("raw_data.csv")

# remove training trials
d = d[ , -grep( "train", names(d) ) ]

# number of real, non-training faces
n.stim = 10

# stimulus names
stim.names = paste( "face.", 1:n.stim, sep="" )


############################### CHECK FOR MISSING DATA ###############################

# the questionnaire is set up so that skipping questions is impossible
# but it's still possible for subjects to stop partway through

# important: this needs to be created as a global variable with this name
# because the subsequent functions in the next code section
# will populate it with subjects that should be excluded
exclusions = data.frame()

# recode missing data
d[ d == "" ] = NA

# check for missing data where there should not be any
cat.names = names(d)[ grep( "_cat", names(d) ) ]
cant.be.na = c( "ResponseId",
                "onReadyTime",
                "buttonClickTime",
                "xPos",
                "yPos",
                "t",
                cat.names )

# mean missingness by variable
apply( d[ ,names(d) %in% cant.be.na ], 2, function(x) mean(is.na(x) ) )

# row numbers for subjects with any missing data
has.na = apply( d[ ,names(d) %in% cant.be.na ], 1, function(x) any( is.na(x) ) )

# exclude them
exclusions = rbind( exclusions, data.frame( ResponseId = d$ResponseId[has.na],
                                            reason = "Missing entire cells of data") )

d = d[ !d$ResponseId %in% exclusions$ResponseId, ]


############################### MAKE STIMULUS-URL KEY FOR RANDOMIZED LOOP & MERGE ###############################

make_url_key( dat = d,
             n.stim = n.stim,
             stim.names = stim.names,
            lm.varname = "cat", 
            key.dir = key.dir )

# read in the key 
setwd(key.dir)
key = read.csv("autogenerated_stimulus_vs_url_key.csv")

# remove extra header rows that we used for making key above
d = d[3:nrow(d),]

# sample size
( initial.n = nrow(d) )


####################### TOTAL MINUTES SUBJECTS SPENT AND HOURLY PAY RATE #######################

# mean experiment duration in minutes
summary( as.numeric( d[["Duration (in seconds)"]] ) / 60 )

# median hourly pay rate
min = median( as.numeric( d[["Duration (in seconds)"]] ) / 60 )
# amount we paid on MTurk
cents = 0.25
( 60 / min ) * cents # hourly pay rate



############################### REMOVE BAD DATA ###############################

# this requires actually trying to make the subject lists for 1 coordinate variable
#  in order to check for problems

# dry run to see which subjects need to be excluded
invisible( get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = TRUE,
                        key = key ) )

# see how many need to be excluded for each reason
table(exclusions$reason)

# remove them
d = d[ !d$ResponseId %in% exclusions$ResponseId, ]



############################### WHAT ALERTS DID SUBJECTS GET? ###############################

alerts = describe_alerts( dat = d,
                          n.stim = n.stim,
                          key = key )



############################### PARSE CURSOR DATA ###############################

# lists for each of the three spacetime variables
# these will be ordered correctly, not randomized

# so the [[i]][[j]] entry of each of these is the ith subject's vector
#  of values for stimulus j (where j is the order in the key, not the randomized order
#  it was shown)

# rescale so that every face trajectory has length 1 in x and y direction

# x-coordinates
xl = get_subject_lists( data = d, 
                        var.name = "xPos",
                        reorder = TRUE,
                        key = key,
                        rescale = TRUE )

# y-coordinates
yl = get_subject_lists( data = d, 
                        var.name = "yPos",
                        reorder = TRUE,
                        key = key,
                        rescale = TRUE )

# time coordinates
tl = get_subject_lists( data = d, 
                        var.name = "time",
                        reorder = TRUE,
                        key = key )

# add outcome variables to wide-format data
d = add_outcomes( d, xl, yl, tl )


# save R objects
setwd(data.dir)
save( xl, file = "xl_subject_lists.RData" )
save( yl, file = "yl_subject_lists.RData" )
save( tl, file = "tl_subject_lists.RData" )



####################### NUISANCE COVARIATES THAT WILL BE ADJUSTED IN ANALYSIS #######################

# identify subjects who ever had window too small
rows = grep( "4", d$alerts )
# make indicator variable for having window too small
d$wts = FALSE
d$wts[ rows ] = TRUE
table(d$wts)

# identify subjects with pixel scaling issues
weird.scalers = exclusions$ResponseId[ exclusions$reason == "Nonstandard pixel dimensions." ]
d$weird.scaling = FALSE
d$weird.scaling[ d$ResponseId %in% weird.scalers ] = TRUE 
table(d$weird.scaling)



####################### MORE SUBJECT EXCLUSIONS #######################

# if you want to exclude any additional subjects, do it here

# save final wide dataset 
setwd(data.dir)
write.csv( d, "wide_data_prepped.csv", row.names = FALSE )

# final sample size
nrow(d)


####################### RESHAPE INTO LONG FORM #######################

# for analysis, we want a long-form dataset with 1 row per trial
# each row represents one unique stimulus and subject combo

l = wide_to_long( dat = d, 
                  stim.names = stim.names )

setwd(data.dir)
write.csv(l, "long_data_prepped.csv")

# write all other R objects as well
setwd(results.dir)
save.image( file = "all_data_prep_objects.RData")


